{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_rows, num_cols = 2, 5  # You can change these values to display more or fewer images\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i in range(num_rows * num_cols):\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Train Data Shape: (60000, 28, 28)\n",
      "Normalized Test Data Shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import normalize\n",
    "\n",
    "X_train_normalized = normalize(X_train, axis=1)\n",
    "X_test_normalized = normalize(X_test, axis=1)\n",
    "\n",
    "print(\"Normalized Train Data Shape:\", X_train_normalized.shape)\n",
    "print(\"Normalized Test Data Shape:\", X_test_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Train Data Shape: (60000, 28, 28, 1)\n",
      "Expanded Test Data Shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_expanded = expand_dims(X_train_normalized, axis=-1)\n",
    "X_test_expanded = expand_dims(X_test_normalized, axis=-1)\n",
    "\n",
    "print(\"Expanded Train Data Shape:\", X_train_expanded.shape)\n",
    "print(\"Expanded Test Data Shape:\", X_test_expanded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot Encoded Train Labels Shape: (60000, 10)\n",
      "One-hot Encoded Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train_expanded = expand_dims(X_train_normalized, axis=-1)\n",
    "X_test_expanded = expand_dims(X_test_normalized, axis=-1)\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"One-hot Encoded Train Labels Shape:\", y_train_cat.shape)\n",
    "print(\"One-hot Encoded Test Labels Shape:\", y_test_cat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 12, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5770      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4, 4), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    ### Model compilation\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in the first convolutional layer: 136\n"
     ]
    }
   ],
   "source": [
    "conv1_layer = model.layers[0]\n",
    "\n",
    "num_filters = conv1_layer.filters\n",
    "filter_size = conv1_layer.kernel_size[0]\n",
    "num_input_channels = conv1_layer.input_shape[-1]\n",
    "\n",
    "params_conv1 = (filter_size * filter_size * num_input_channels + 1) * num_filters\n",
    "\n",
    "print(\"Number of trainable parameters in the first convolutional layer:\", params_conv1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([48572, 38696, 13611, ...,   860, 15795, 56422])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/reecepalmer/Code/RPalmr/06-Deep-Learning/03-Convolutional-Neural-Networks/data-mnist-classification/mnist_classification.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/reecepalmer/Code/RPalmr/06-Deep-Learning/03-Convolutional-Neural-Networks/data-mnist-classification/mnist_classification.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_train_cat \u001b[39m=\u001b[39m to_categorical(y_train, num_classes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reecepalmer/Code/RPalmr/06-Deep-Learning/03-Convolutional-Neural-Networks/data-mnist-classification/mnist_classification.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Split the data into training and validation sets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/reecepalmer/Code/RPalmr/06-Deep-Learning/03-Convolutional-Neural-Networks/data-mnist-classification/mnist_classification.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X_train_split, X_val_split, y_train_split, y_val_split \u001b[39m=\u001b[39m train_test_split(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reecepalmer/Code/RPalmr/06-Deep-Learning/03-Convolutional-Neural-Networks/data-mnist-classification/mnist_classification.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     X_train_expanded, y_train_cat, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reecepalmer/Code/RPalmr/06-Deep-Learning/03-Convolutional-Neural-Networks/data-mnist-classification/mnist_classification.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reecepalmer/Code/RPalmr/06-Deep-Learning/03-Convolutional-Neural-Networks/data-mnist-classification/mnist_classification.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model \u001b[39m=\u001b[39m initialize_model()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reecepalmer/Code/RPalmr/06-Deep-Learning/03-Convolutional-Neural-Networks/data-mnist-classification/mnist_classification.ipynb#X50sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Define early stopping\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2672\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2668\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m   2670\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[0;32m-> 2672\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m   2673\u001b[0m     chain\u001b[39m.\u001b[39;49mfrom_iterable(\n\u001b[1;32m   2674\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m arrays\n\u001b[1;32m   2675\u001b[0m     )\n\u001b[1;32m   2676\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2674\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2668\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m   2670\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[1;32m   2672\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2673\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[0;32m-> 2674\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m   2675\u001b[0m     )\n\u001b[1;32m   2676\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/utils/__init__.py:355\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39maxis)\n\u001b[1;32m    354\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m    356\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/utils/__init__.py:184\u001b[0m, in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    183\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m--> 184\u001b[0m \u001b[39mreturn\u001b[39;00m array[key] \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m array[:, key]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:906\u001b[0m, in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    901\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(idx, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    902\u001b[0m \u001b[39mif\u001b[39;00m (dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m dtypes\u001b[39m.\u001b[39mas_dtype(dtype) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _SUPPORTED_SLICE_DTYPES \u001b[39mor\u001b[39;00m\n\u001b[1;32m    903\u001b[0m     idx\u001b[39m.\u001b[39mshape \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(idx\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    904\u001b[0m   \u001b[39m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[39;00m\n\u001b[1;32m    905\u001b[0m   \u001b[39m# will break `_slice_helper` contract.\u001b[39;00m\n\u001b[0;32m--> 906\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(_SLICE_TYPE_ERROR \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(idx))\n",
      "\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([48572, 38696, 13611, ...,   860, 15795, 56422])"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_normalized = normalize(X_train, axis=1)\n",
    "X_train_expanded = expand_dims(X_train_normalized, axis=-1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_expanded, y_train_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Fit the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/313 [..............................] - ETA: 59s - loss: 2.2910 - accuracy: 0.0625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 13:33:55.674075: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3044 - accuracy: 0.0889\n",
      "Accuracy on the test set: 8.89%\n"
     ]
    }
   ],
   "source": [
    "X_test_normalized = normalize(X_test, axis=1)\n",
    "X_test_expanded = expand_dims(X_test_normalized, axis=-1)\n",
    "\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_expanded, y_test_cat)\n",
    "\n",
    "print(f\"Accuracy on the test set: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
